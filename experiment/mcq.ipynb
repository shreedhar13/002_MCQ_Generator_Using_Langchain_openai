{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pandas\n",
    "import traceback\n",
    "import langchain \n",
    "import os\n",
    "from langchain.chains import LLMChain \n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain import PromptTemplate , HuggingFaceHub , LLMChain\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "#from langchain.llms import OpenAI,,,,,,it is also same as above,,,,,,,,,but above is latest one so,use this\n",
    "#for this task we want Text-2-Text generation llm models,,,\n",
    "#and i will use google/flan-t5-xxl,,, this model from huggingface,,bcz openai is not allowing my card for payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\": 0.7})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating Input prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "Text:{text}\n",
    "you are an expert MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz of {number} multiple choice questions for {subject} students in {tone} tone.\n",
    "Make sure the questions are not repeated and check all the questions to be conforming the next as well.\n",
    "Make sure to format your response like RESPONSE_JSON below and use it as a guide.\\\n",
    "Ensure to make {number} MCQs.\n",
    "RESPONSE_JSON {response_json}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables = [\"text\",'number','subject','tone','response_json'] ,\n",
    "    template = TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain-1\n",
    "quiz_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = quiz_generation_prompt,\n",
    "    output_key=\"quiz\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating Output Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    \"1\":{\n",
    "        \"mcq\":\"Multiple choice question\",\n",
    "        \"options\":{\n",
    "            'a':'choice here',\n",
    "            'b':'choice here',\n",
    "            'c':'choice here',\n",
    "            'd':'choice here',\n",
    "        },\n",
    "        'correct':'correct option',\n",
    "    },\n",
    "    \"2\":{\n",
    "        \"mcq\":\"Multiple choice question\",\n",
    "        \"options\":{\n",
    "            'a':'choice here',\n",
    "            'b':'choice here',\n",
    "            'c':'choice here',\n",
    "            'd':'choice here',\n",
    "        },\n",
    "        'correct':'correct option',\n",
    "    },\n",
    "    \"3\":{\n",
    "        \"mcq\":\"Multiple choice question\",\n",
    "        \"options\":{\n",
    "            'a':'choice here',\n",
    "            'b':'choice here',\n",
    "            'c':'choice here',\n",
    "            'd':'choice here',\n",
    "        },\n",
    "        'correct':'correct option',\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template For evaluating generated quiz is grammatically correct ot not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2 = \"\"\"\n",
    "You are a expert english grammarian and writter. Given a MCQs for {subject} students.\\\n",
    "You need to evaluate the complexity of the questions and give a complete analysis of the quiz.Onlu use atmost 30 words for complexity.\\\n",
    "if the quiz is not as per with the cognitive and analytical abilities of the students,\\\n",
    "update thequiz questions which needs to be changed and change the tone such that it perfectly fits the student ability.\\\n",
    "Quiz MCQ:\n",
    "{quiz}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt = PromptTemplate(\n",
    "    input_variables=['subject','quiz'],\n",
    "    template=TEMPLATE2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain-2\n",
    "review_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = quiz_evaluation_prompt,\n",
    "    output_key=\"review\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Connect chain-1 and chain-2 using **SequentialChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain = SequentialChain(\n",
    "        chains=[quiz_chain,review_chain],\n",
    "        input_variables=['text','number','subject','tone','response_json'],\n",
    "        output_variables=['quiz','review'],\n",
    "        verbose=True\n",
    ")\n",
    "#first quiz chain is executed then result of it is stored in 'quiz' variable and then passed to review_chain ,here it will execute using 'quiz' and 'subject' variable\n",
    "#and its result is stored in 'review' variable...\n",
    "#subject is automaically inferred by gen ai model,bcz they are smart enough,,what we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Purpose: SimpleSequentialChain** is designed for chaining multiple LLMChain instances in a straightforward sequence where each chain’s output is passed to the next one. It is simpler and less flexible than SequentialChain.\n",
    "- **Purpose: SequentialChain** is more flexible and powerful than SimpleSequentialChain. It allows for more complex data flow between chains, including mapping and transforming inputs/outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Read PDF file and give it as text to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(r\"D:\\GENERATIVE-AI\\004_Projects\\002_MCQ_Generator_Using_Langchain_openai\\data.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\GENERATIVE-AI\\\\004_Projects\\\\002_MCQ_Generator_Using_Langchain_openai\\\\data.pdf', 'page': 0}, page_content='There is a close connection between machine learning and compression. A system that \\npredicts the  posterior probabilities  of a sequence given its entire history can be used for \\noptimal data compression (by using  arithmetic coding  on the output distribution). \\nConversely, an optimal compressor can be used for prediction (by finding the symbol that \\ncompresses best, given the previous history). This equivalence has been used as a \\njustification for using data compression as a benchmark  for \"general intelligence\".   \\nAn alternative view can show compression algorithms implicitly map strings into \\nimplicit  feature space vectors , and compression -based similarity measures compute \\nsimilarity within these feature spaces. For each compressor C(.) we define an associated \\nvector space ℵ, such that C(.) maps an input string x, corresponding to the vector norm \\n||~x||. An exhaustive examination of the feature spaces underlying all compression \\nalgorithms is precluded by space; instead, feature vectors chooses to examine three \\nrepresentative lossless compression methods, LZW, LZ77, and PPM.   \\nAccording to  AIXI theory, a connection more directly explained in  Hutter Prize , the best \\npossible compression of x is the smallest possible software that generates x. For example, \\nin that model, a zip file\\'s compressed size includes both the zip file and the unzipping \\nsoftware, since you can not unzip it without both, but there may be an even smaller \\ncombined form.  \\nExamples of AI -powered audio/video compression software include  NVIDIA Maxine , \\nAIVC.[29] Examples of software that can perform AI -powered image compression \\ninclude  OpenCV , TensorFlow , MATLAB \\'s Image Processing Toolbox (IPT) and High -\\nFidelity Generative Image Compression.   \\nIn unsupervised machine learning , k-means clustering  can be utilized to compress data by \\ngrouping similar data points into clusters. This technique simplifies handling extensive \\ndatasets that lack predefined labels and finds widespread use in fields such as  image \\ncompression .  \\nData compression aims to reduce the size of data files, enhancing storage efficiency and \\nspeeding up data transmission. K -means clustering, an unsupervised machine learning \\nalgorithm, is employed to partition a dataset into a specified number of clusters, k, each \\nrepresented by the  centroid  of its points. This process condenses extensive datasets into \\na more compact set of representative points. Particularly beneficial in  image  and signal \\nprocessing , k-means clustering aids in data reduction by replacing groups of data points \\nwith their centroids, thereby preserving the core information of the original data while \\nsignificantly decreasing the required storage space.   \\nLarge language models  (LLMs) are also capable of lossless data compression, as \\ndemonstrated by  DeepMind \\'s research with the Chinchilla 70B model. Developed by \\nDeepMind, Chinchilla 70B effectively compressed data, outperforming conventional \\nmethods such as  Portable Network Graphics  (PNG) for images and  Free Lossless Audio \\nCodec  (FLAC) for audio. It achieved compression of image and audio data to 43.4% and \\n16.4% of their original sizes, respectively.')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  page in range(len(pages)):\n",
    "    TEXT += pages[page].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is a close connection between machine learning and compression. A system that \\npredicts the  posterior probabilities  of a sequence given its entire history can be used for \\noptimal data compression (by using  arithmetic coding  on the output distribution). \\nConversely, an optimal compressor can be used for prediction (by finding the symbol that \\ncompresses best, given the previous history). This equivalence has been used as a \\njustification for using data compression as a benchmark  for \"general intelligence\".   \\nAn alternative view can show compression algorithms implicitly map strings into \\nimplicit  feature space vectors , and compression -based similarity measures compute \\nsimilarity within these feature spaces. For each compressor C(.) we define an associated \\nvector space ℵ, such that C(.) maps an input string x, corresponding to the vector norm \\n||~x||. An exhaustive examination of the feature spaces underlying all compression \\nalgorithms is precluded by space; instead, feature vectors chooses to examine three \\nrepresentative lossless compression methods, LZW, LZ77, and PPM.   \\nAccording to  AIXI theory, a connection more directly explained in  Hutter Prize , the best \\npossible compression of x is the smallest possible software that generates x. For example, \\nin that model, a zip file\\'s compressed size includes both the zip file and the unzipping \\nsoftware, since you can not unzip it without both, but there may be an even smaller \\ncombined form.  \\nExamples of AI -powered audio/video compression software include  NVIDIA Maxine , \\nAIVC.[29] Examples of software that can perform AI -powered image compression \\ninclude  OpenCV , TensorFlow , MATLAB \\'s Image Processing Toolbox (IPT) and High -\\nFidelity Generative Image Compression.   \\nIn unsupervised machine learning , k-means clustering  can be utilized to compress data by \\ngrouping similar data points into clusters. This technique simplifies handling extensive \\ndatasets that lack predefined labels and finds widespread use in fields such as  image \\ncompression .  \\nData compression aims to reduce the size of data files, enhancing storage efficiency and \\nspeeding up data transmission. K -means clustering, an unsupervised machine learning \\nalgorithm, is employed to partition a dataset into a specified number of clusters, k, each \\nrepresented by the  centroid  of its points. This process condenses extensive datasets into \\na more compact set of representative points. Particularly beneficial in  image  and signal \\nprocessing , k-means clustering aids in data reduction by replacing groups of data points \\nwith their centroids, thereby preserving the core information of the original data while \\nsignificantly decreasing the required storage space.   \\nLarge language models  (LLMs) are also capable of lossless data compression, as \\ndemonstrated by  DeepMind \\'s research with the Chinchilla 70B model. Developed by \\nDeepMind, Chinchilla 70B effectively compressed data, outperforming conventional \\nmethods such as  Portable Network Graphics  (PNG) for images and  Free Lossless Audio \\nCodec  (FLAC) for audio. It achieved compression of image and audio data to 43.4% and \\n16.4% of their original sizes, respectively.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Serialize the python dictionary into a JSON Formatted string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"1\": {\"mcq\": \"Multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct option\"}, \"2\": {\"mcq\": \"Multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct option\"}, \"3\": {\"mcq\": \"Multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct option\"}}'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(RESPONSE_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. OpenAI Callbacks functionality cloning in to HuggingFace, For **setup Token usage Tracking in Langchain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from langchain.callbacks import get_openai_callback -> it is for openai\n",
    "- there is no any thing like -> from langchain.callbacks import get_huggingface_callback\n",
    "- you have to manually code it by **inheriting BaseCallbackManager** Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For OpenAI\n",
    "#to setup token usage tracking in langchain\n",
    "# with get_openai_callback() as cb:\n",
    "#     response = generate_evaluate_chain(\n",
    "#         {\n",
    "#             \"text\":TEXT,\n",
    "#             \"number\":NUMBER,\n",
    "#             \"subject\":SUBJECT,\n",
    "#             \"tone\":TONE,\n",
    "#             \"response_json\":json.dumps(RESPONSE_JSON)\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#response contains output w.r.t given prompt\n",
    "\n",
    "#print(f\"Total Tokens:{cb.total_tokens}\")  -> prompt_token + completion_token\n",
    "#print(f\"Prompt Tokens:{cb.prompt_tokens}\")\n",
    "#print(f\"Completion Tokens:{cb.completion_tokens}\")\n",
    "#print(f\"Total cost:{cb.total_cost}\")  -> total cost estimated to get above response (it includes all charges,context window, total tokens..etc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBERS = 5\n",
    "SUBJECT = \"Data Science\"\n",
    "TONE = \"Hard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
