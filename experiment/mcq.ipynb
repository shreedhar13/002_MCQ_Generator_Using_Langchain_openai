{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pandas\n",
    "import traceback\n",
    "import langchain \n",
    "import os\n",
    "from langchain.chains import LLMChain \n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain import PromptTemplate , HuggingFaceHub , LLMChain\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "#from langchain.llms import OpenAI,,,,,,it is also same as above,,,,,,,,,but above is latest one so,use this\n",
    "#for this task we want Text-2-Text generation llm models,,,\n",
    "#and i will use google/flan-t5-xxl,,, this model from huggingface,,bcz openai is not allowing my card for payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(repo_id=\"google/flan-t5-base\", model_kwargs={\"temperature\": 0.7})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating Input prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "Text:{text}\n",
    "you are an expert MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz of {number} multiple choice questions for {subject} students in {tone} tone.\n",
    "Make sure the questions are not repeated and check all the questions to be conforming the next as well.\n",
    "Make sure to format your response like RESPONSE_JSON below and use it as a guide.\\\n",
    "Ensure to make {number} MCQs.\n",
    "RESPONSE_JSON {response_json}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables = [\"text\",'number','subject','tone','response_json'] ,\n",
    "    template = TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain-1\n",
    "quiz_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = quiz_generation_prompt,\n",
    "    output_key=\"quiz\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating Output Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    \"1\":{\n",
    "        \"mcq\":\"Multiple choice question\",\n",
    "        \"options\":{\n",
    "            'a':'choice here',\n",
    "            'b':'choice here',\n",
    "            'c':'choice here',\n",
    "            'd':'choice here',\n",
    "        },\n",
    "        'correct':'correct option',\n",
    "    },\n",
    "    \"2\":{\n",
    "        \"mcq\":\"Multiple choice question\",\n",
    "        \"options\":{\n",
    "            'a':'choice here',\n",
    "            'b':'choice here',\n",
    "            'c':'choice here',\n",
    "            'd':'choice here',\n",
    "        },\n",
    "        'correct':'correct option',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template For evaluating generated quiz is grammatically correct ot not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2 = \"\"\"\n",
    "You are a expert english grammarian and writter. Given a MCQs for {subject} students.\\\n",
    "You need to evaluate the complexity of the questions and give a complete analysis of the quiz.Onlu use atmost 30 words for complexity.\\\n",
    "if the quiz is not as per with the cognitive and analytical abilities of the students,\\\n",
    "update thequiz questions which needs to be changed and change the tone such that it perfectly fits the student ability.\\\n",
    "Quiz MCQ:\n",
    "{quiz}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt = PromptTemplate(\n",
    "    input_variables=['subject','quiz'],\n",
    "    template=TEMPLATE2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain-2\n",
    "review_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = quiz_evaluation_prompt,\n",
    "    output_key=\"review\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Connect chain-1 and chain-2 using **SequentialChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain = SequentialChain(\n",
    "        chains=[quiz_chain,review_chain],\n",
    "        input_variables=['text','number','subject','tone','response_json'],\n",
    "        output_variables=['quiz','review'],\n",
    "        verbose=True\n",
    ")\n",
    "#first quiz chain is executed then result of it is stored in 'quiz' variable and then passed to review_chain ,here it will execute using 'quiz' and 'subject' variable\n",
    "#and its result is stored in 'review' variable...\n",
    "#subject is automaically inferred by gen ai model,bcz they are smart enough,,what we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Purpose: SimpleSequentialChain** is designed for chaining multiple LLMChain instances in a straightforward sequence where each chain’s output is passed to the next one. It is simpler and less flexible than SequentialChain.\n",
    "- **Purpose: SequentialChain** is more flexible and powerful than SimpleSequentialChain. It allows for more complex data flow between chains, including mapping and transforming inputs/outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Read PDF file and give it as text to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(r\"D:\\GENERATIVE-AI\\004_Projects\\002_MCQ_Generator_Using_Langchain_openai\\data2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\GENERATIVE-AI\\\\004_Projects\\\\002_MCQ_Generator_Using_Langchain_openai\\\\data2.pdf', 'page': 0}, page_content='There is a close connection between machine learning and compression. A system that \\npredicts the  posterior probabilities  of a sequence given its entire history can be used for \\noptimal data compression (by using  arithmetic coding  on the output distribution). \\nConversely, an optimal compressor can be used for prediction (by finding the symbol that \\ncompresses best, given the previous history). This equivalence has been used as a \\njustification for using data compression as a benchmark for \"general intelligence\".   \\nAn alternative view can show compression algorithms implicitly map strings into implicit  \\nfeature space vectors , and compression -based similarity measures compute similarity \\nwithin these feature spaces. For each compressor C(.) we define an associated vector \\nspace ℵ, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An \\nexhaustive examination of the feature spaces underlying all compression algorithms is \\nprecluded by space; instead, feature vectors chooses to examine three representative \\nlossless compression methods, LZW, LZ77, and PPM.')]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  page in range(len(pages)):\n",
    "    TEXT += pages[page].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is a close connection between machine learning and compression. A system that \\npredicts the  posterior probabilities  of a sequence given its entire history can be used for \\noptimal data compression (by using  arithmetic coding  on the output distribution). \\nConversely, an optimal compressor can be used for prediction (by finding the symbol that \\ncompresses best, given the previous history). This equivalence has been used as a \\njustification for using data compression as a benchmark for \"general intelligence\".   \\nAn alternative view can show compression algorithms implicitly map strings into implicit  \\nfeature space vectors , and compression -based similarity measures compute similarity \\nwithin these feature spaces. For each compressor C(.) we define an associated vector \\nspace ℵ, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An \\nexhaustive examination of the feature spaces underlying all compression algorithms is \\nprecluded by space; instead, feature vectors chooses to examine three representative \\nlossless compression methods, LZW, LZ77, and PPM.'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Serialize the python dictionary into a JSON Formatted string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"1\": {\"mcq\": \"Multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct option\"}, \"2\": {\"mcq\": \"Multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct option\"}, \"3\": {\"mcq\": \"Multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct option\"}}'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(RESPONSE_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Calling the model/Executing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBERS = 5\n",
    "SUBJECT = \"Data Science\"\n",
    "TONE = \"simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:There is a close connection between machine learning and compression. A system that \n",
      "predicts the  posterior probabilities  of a sequence given its entire history can be used for \n",
      "optimal data compression (by using  arithmetic coding  on the output distribution). \n",
      "Conversely, an optimal compressor can be used for prediction (by finding the symbol that \n",
      "compresses best, given the previous history). This equivalence has been used as a \n",
      "justification for using data compression as a benchmark for \"general intelligence\".   \n",
      "An alternative view can show compression algorithms implicitly map strings into implicit  \n",
      "feature space vectors , and compression -based similarity measures compute similarity \n",
      "within these feature spaces. For each compressor C(.) we define an associated vector \n",
      "space ℵ, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An \n",
      "exhaustive examination of the feature spaces underlying all compression algorithms is \n",
      "precluded by space; instead, feature vectors chooses to examine three representative \n",
      "lossless compression methods, LZW, LZ77, and PPM.\n",
      "you are an expert MCQ maker. Given the above text, it is your job to create a quiz of 5 multiple choice questions for Data Science students in simple tone.\n",
      "Make sure the questions are not repeated and check all the questions to be conforming the next as well.\n",
      "Make sure to format your response like RESPONSE_JSON below and use it as a guide.Ensure to make 5 MCQs.\n",
      "RESPONSE_JSON {\"1\": {\"mcq\": \"Multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct option\"}, \"2\": {\"mcq\": \"Multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct option\"}}\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a expert english grammarian and writter. Given a MCQs for Data Science students.You need to evaluate the complexity of the questions and give a complete analysis of the quiz.Onlu use atmost 30 words for complexity.if the quiz is not as per with the cognitive and analytical abilities of the students,update thequiz questions which needs to be changed and change the tone such that it perfectly fits the student ability.Quiz MCQ:\n",
      "Which of the following is not a feature space vector: LZW, LZ77\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = generate_evaluate_chain({\"text\":TEXT , \"number\":NUMBERS , \"subject\":SUBJECT , \"tone\":TONE , \"response_json\":json.dumps(RESPONSE_JSON)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'There is a close connection between machine learning and compression. A system that \\npredicts the  posterior probabilities  of a sequence given its entire history can be used for \\noptimal data compression (by using  arithmetic coding  on the output distribution). \\nConversely, an optimal compressor can be used for prediction (by finding the symbol that \\ncompresses best, given the previous history). This equivalence has been used as a \\njustification for using data compression as a benchmark for \"general intelligence\".   \\nAn alternative view can show compression algorithms implicitly map strings into implicit  \\nfeature space vectors , and compression -based similarity measures compute similarity \\nwithin these feature spaces. For each compressor C(.) we define an associated vector \\nspace ℵ, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An \\nexhaustive examination of the feature spaces underlying all compression algorithms is \\nprecluded by space; instead, feature vectors chooses to examine three representative \\nlossless compression methods, LZW, LZ77, and PPM.',\n",
       " 'number': 5,\n",
       " 'subject': 'Data Science',\n",
       " 'tone': 'simple',\n",
       " 'response_json': '{\"1\": {\"mcq\": \"Multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct option\"}, \"2\": {\"mcq\": \"Multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct option\"}}',\n",
       " 'quiz': 'Which of the following is not a feature space vector: LZW, LZ77',\n",
       " 'review': 'LZW is not a feature space vector.'}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"1\": {\"mcq\": \"Multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct option\"}, \"2\": {\"mcq\": \"Multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct option\"}}'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['response_json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'mcq': 'Multiple choice question',\n",
       "  'options': {'a': 'choice here',\n",
       "   'b': 'choice here',\n",
       "   'c': 'choice here',\n",
       "   'd': 'choice here'},\n",
       "  'correct': 'correct option'},\n",
       " '2': {'mcq': 'Multiple choice question',\n",
       "  'options': {'a': 'choice here',\n",
       "   'b': 'choice here',\n",
       "   'c': 'choice here',\n",
       "   'd': 'choice here'},\n",
       "  'correct': 'correct option'}}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response['response_json'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. OpenAI Callbacks functionality cloning in to HuggingFace, For **setup Token usage Tracking in Langchain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from langchain.callbacks import get_openai_callback -> it is for openai\n",
    "- there is no any thing like -> from langchain.callbacks import get_huggingface_callback\n",
    "- you have to manually code it by **inheriting BaseCallbackManager** Class ro **AutoTokenizer** class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For OpenAI\n",
    "#to setup token usage tracking in langchain\n",
    "# with get_openai_callback() as cb:\n",
    "#     response = generate_evaluate_chain(\n",
    "#         {\n",
    "#             \"text\":TEXT,\n",
    "#             \"number\":NUMBER,\n",
    "#             \"subject\":SUBJECT,\n",
    "#             \"tone\":TONE,\n",
    "#             \"response_json\":json.dumps(RESPONSE_JSON)\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#response contains output w.r.t given prompt\n",
    "\n",
    "# print(f\"Total Tokens:{cb.total_tokens}\")  -> prompt_token + completion_token\n",
    "# print(f\"Prompt Tokens:{cb.prompt_tokens}\")\n",
    "# print(f\"Completion Tokens:{cb.completion_tokens}\")\n",
    "# print(f\"Total cost:{cb.total_cost}\")  -> total cost estimated to get above response (it includes all charges,context window, total tokens..etc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 339\n",
      "prompt tokens: 229\n",
      "completion tokens: 110\n",
      "total cost: 0.0002034  Dollars\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(text, tokenizer):\n",
    "    # Encode the text to get token IDs\n",
    "    encoding = tokenizer.encode(text)\n",
    "    # Return the number of tokens\n",
    "    return len(encoding)\n",
    "\n",
    "# Example usage\n",
    "text = TEXT\n",
    "prompt_tokens = count_tokens(text, tokenizer)\n",
    "completion_tokens = 110   # count_tokens(json.loads(response['response_json']), tokenizer)\n",
    "total_tokens = prompt_tokens + completion_tokens\n",
    "   \n",
    "# Estimate cost (example values, adjust based on actual pricing)\n",
    "cost_per_1k_tokens = 0.0006  # Example cost per 1,000 tokens (change as needed)\n",
    "total_tokens = prompt_tokens + completion_tokens\n",
    "total_cost = round((total_tokens / 1000) * cost_per_1k_tokens , 7)\n",
    "\n",
    "\n",
    "print(f\"Total tokens: {total_tokens}\")\n",
    "print(f\"prompt tokens: {prompt_tokens}\")\n",
    "print(f\"completion tokens: {completion_tokens}\")\n",
    "print(f\"total cost: {total_cost}\"+\"  Dollars\")\n",
    "\n",
    "#Prompt tokens according to \"google/flan-t5-large\" this model is 690,,,,,,,but in open ai it is 706 tokens.............\n",
    "#so we conclude that,,each model has it own tokenizer mechanism,,which works differently from others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
